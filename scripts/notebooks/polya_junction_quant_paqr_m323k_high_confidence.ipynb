{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying poly(A) sites as junction counts relative to 5' TE splice junction\n",
    "\n",
    "\n",
    "The throw it out there idea is that if this is reliable (i.e. just considering counts across poly(A) sites (accounting for minimum overhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chr', 'cluster_start', 'cluster_end', 'site_id', 'score', 'strand', 'n_along_exon', 'total_sites_on_exon', 'paqr_exon_id', 'gene_id', 'M323K_WT_1', 'M323K_HOM_1', 'M323K_WT_2', 'M323K_HOM_2', 'M323K_WT_3', 'M323K_HOM_3', 'M323K_WT_4', 'M323K_HOM_4', 'M323K_WT_1', 'M323K_HOM_5']\n",
      "{'M323K_HOM_4', 'M323K_WT_1', 'M323K_HOM_5', 'M323K_HOM_1', 'M323K_HOM_3', 'M323K_WT_2', 'M323K_WT_4', 'M323K_HOM_2', 'M323K_WT_3'}\n",
      "        chr  cluster_start  cluster_end               site_id  score strand  \\\n",
      "0     chr16       20544628     20544662   chr16:+:20544651:TE      4      +   \n",
      "1      chrX      133931787    133931831   chrX:+:133931813:TE      7      +   \n",
      "2      chrX      133932295    133932296   chrX:+:133932296:TE      1      +   \n",
      "3     chr17       75390823     75390843   chr17:+:75390843:TE      1      +   \n",
      "4     chr17       75391754     75391779   chr17:+:75391769:TE      8      +   \n",
      "...     ...            ...          ...                   ...    ...    ...   \n",
      "3498   chr1      171388592    171388593   chr1:+:171388593:TE      1      +   \n",
      "3499  chr18       10817819     10817820   chr18:+:10817820:TE      1      +   \n",
      "3500  chr18       10818545     10818591   chr18:+:10818572:TE      9      +   \n",
      "3501  chr11      105281885    105281910  chr11:+:105281900:TE      9      +   \n",
      "3502  chr11      105282139    105282187  chr11:+:105282176:TE      9      +   \n",
      "\n",
      "      n_along_exon  total_sites_on_exon  \\\n",
      "0                2                    2   \n",
      "1                1                    2   \n",
      "2                2                    2   \n",
      "3                1                    2   \n",
      "4                2                    2   \n",
      "...            ...                  ...   \n",
      "3498             2                    2   \n",
      "3499             1                    2   \n",
      "3500             2                    2   \n",
      "3501             1                    2   \n",
      "3502             2                    2   \n",
      "\n",
      "                                      paqr_exon_id             gene_id  \\\n",
      "0       ENSMUST00000007216:12:12:20543310:20544909  ENSMUST00000007216   \n",
      "1     ENSMUST00000113304:11:11:133931285:133932446  ENSMUST00000113304   \n",
      "2     ENSMUST00000113304:11:11:133931285:133932446  ENSMUST00000113304   \n",
      "3       ENSMUST00000001927:34:34:75390371:75391767  ENSMUST00000001927   \n",
      "4       ENSMUST00000001927:34:34:75390371:75391767  ENSMUST00000001927   \n",
      "...                                            ...                 ...   \n",
      "3498    ENSMUST00000006578:9:9:171386580:171388598  ENSMUST00000006578   \n",
      "3499    ENSMUST00000124288:15:15:10817818:10818704  ENSMUST00000124288   \n",
      "3500    ENSMUST00000124288:15:15:10817818:10818704  ENSMUST00000124288   \n",
      "3501  ENSMUST00000015107:21:21:105281084:105282176  ENSMUST00000015107   \n",
      "3502  ENSMUST00000015107:21:21:105281084:105282176  ENSMUST00000015107   \n",
      "\n",
      "      M323K_WT_1  M323K_HOM_1  M323K_WT_2  M323K_HOM_2  M323K_WT_3  \\\n",
      "0           1.24         0.94        1.01         0.96        1.20   \n",
      "1          77.90        80.29       84.12        87.88       84.79   \n",
      "2          22.10        19.71       15.88        12.12       15.21   \n",
      "3          53.87        57.71       59.07        56.42       54.81   \n",
      "4          46.13        42.29       40.93        43.58       45.19   \n",
      "...          ...          ...         ...          ...         ...   \n",
      "3498        0.00         0.00        0.00         1.28        1.28   \n",
      "3499       73.28         0.00       76.02        69.25       75.55   \n",
      "3500       26.72       100.00       23.98        30.75       24.45   \n",
      "3501       42.12        34.75       32.73        33.24       39.23   \n",
      "3502       57.88        65.25       67.27        66.76       60.77   \n",
      "\n",
      "      M323K_HOM_3  M323K_WT_4  M323K_HOM_4  M323K_WT_1  M323K_HOM_5  \n",
      "0            0.90        0.71         0.99        1.24         1.34  \n",
      "1           88.57       82.14        91.22       77.90        66.57  \n",
      "2           11.43       17.86         8.78       22.10        33.43  \n",
      "3           50.26       52.27        38.13       53.87        46.96  \n",
      "4           49.74       47.73        61.87       46.13        53.04  \n",
      "...           ...         ...          ...         ...          ...  \n",
      "3498         0.00        0.00         1.67        0.00         0.88  \n",
      "3499        79.37       74.77        79.20       73.28        77.80  \n",
      "3500        20.63       25.23        20.80       26.72        22.20  \n",
      "3501        39.73       45.41        45.51       42.12        35.87  \n",
      "3502        60.27       54.59        54.49       57.88        64.13  \n",
      "\n",
      "[3503 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pysam\n",
    "os.getcwd()\n",
    "\n",
    "paqr_rel_usages_path = \"../../data/relative_usages.filtered.tsv\"\n",
    "rel_usages_header_path = \"../../data/relative_usages.header.out\"\n",
    "m323k_wt_1_bam_path = \"/home/sam/cluster/TDP43_RNA/TDP_F210I_M323K/M323K/New_adult_brain/processed/M323K_WT_1/M323K_WT_1_unique_rg_fixed.bam\"\n",
    "m323k_hom_1_bam_path = \"/home/sam/cluster/TDP43_RNA/TDP_F210I_M323K/M323K/New_adult_brain/processed/M323K_HOM_1/M323K_HOM_1_unique_rg_fixed.bam\"\n",
    "\n",
    "rel_usages = pd.read_csv(paqr_rel_usages_path,sep=\"\\t\")\n",
    "\n",
    "with open(rel_usages_header_path) as inpt:\n",
    "    \n",
    "    sample_names = [line.rstrip() for line in inpt]\n",
    "\n",
    "\n",
    "#print(rel_usages)\n",
    "#print(sample_names)\n",
    "\n",
    "#First 10 columns are the same in relative usages df\n",
    "colnames_rel_usages = [\"chr\",\n",
    " \"cluster_start\",\n",
    " \"cluster_end\",\n",
    " \"site_id\",\n",
    " \"score\",\n",
    " \"strand\",\n",
    " \"n_along_exon\",\n",
    " \"total_sites_on_exon\", \n",
    " \"paqr_exon_id\", \n",
    " \"gene_id\" # this is technically transcript_id but for now I'll leave it...\n",
    "]\n",
    "\n",
    "#Rest of columns are samples names in order found in relative usages output df\n",
    "colnames_rel_usages.extend(sample_names)\n",
    "\n",
    "print(colnames_rel_usages)\n",
    "\n",
    "\n",
    "#Because samples are paired according to config - can appear multiple times in df...\n",
    "sample_names = set(sample_names)\n",
    "print(sample_names)\n",
    "\n",
    "rel_usages.columns = colnames_rel_usages\n",
    "print(rel_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#grouped = rel_usages.groupby(\"gene_id\")\n",
    "#print(grouped)\n",
    "\n",
    "#for a, b in grouped:\n",
    "#    print(a)\n",
    "#    print(b[\"site_id\"].to_list())\n",
    "\n",
    "    \n",
    "    \n",
    "def polyASite_id_to_coordinate_tuple(paqr_df, site_id_colname = \"site_id\", group_col = \"gene_id\"):\n",
    "    '''\n",
    "    Nested dict of {<group_col_key>: {site_id: (chr, start, end)}}}\n",
    "    Assume coord is 1 based, output will be 0 based, 1/2 open\n",
    "    '''\n",
    "    \n",
    "    df_grouped = paqr_df.groupby(group_col)\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    for group_name, group in df_grouped:\n",
    "        \n",
    "        site_ids = group[site_id_colname].to_list()\n",
    "        nested_dict = {}\n",
    "        \n",
    "        for site in site_ids:\n",
    "            #'chr11:+:55110898:TE'\n",
    "            ID = site.split(':')\n",
    "            seq_tuple = (ID[0], int(ID[2]), int(ID[2]) + 1)\n",
    "            \n",
    "            nested_dict[site] = seq_tuple \n",
    "            \n",
    "        out_dict[group_name] = nested_dict\n",
    "        \n",
    "        \n",
    "    return out_dict\n",
    "\n",
    "\n",
    "polya_jnc_coords = polyASite_id_to_coordinate_tuple(rel_usages)\n",
    "#print(polya_jnc_coords)\n",
    "\n",
    "\n",
    "### For testing, let's just look at a few transcripts\n",
    "small_polya_jnc_coords = {key: polya_jnc_coords[key] for key in list(polya_jnc_coords.keys())[:5]}\n",
    "#print(small_polya_jnc_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I've got my PAQR inferred/ PolyASite poly(A) sites , I want to try and count the number of reads that span these positions - 'junction reads'\n",
    "\n",
    "As I will want to quantify these junctions relative to the splice junction at the 5'end of the terminal exon, I have to treat the poly(A) junction alignments as if they were a splice junction (where sequence is disjointly aligned to the genome\n",
    "\n",
    "STAR (and likely other splice-aware aligners) has a --alignSJDBoverhangMin parameter, which requires that a putative junction read 'overhangs' the junction by at least x nt (x = 3 by default) in orfer for it to be assigned to the junction\n",
    "\n",
    "\n",
    "Reads aligned to genome at poly(A) sites aren't subject to this parameter, so junction counts would be inflated relative to the 5' splice junction\n",
    "\n",
    "\n",
    "using pysam, for each read crossing the poly(A) site, check whether start or end of read alignment falls within 3nt of the poly(A) site coordinate\n",
    "\n",
    "abs(polya_site - alignment_start) > 3 and abs(alignment_end - polya_site) > 3 = valid poly(A) site junction read\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': 3, 'chr11:+:55113026:TE': 1}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': 212, 'chr2:+:181515774:TE': 102, 'chr2:+:181516756:TE': 26}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': 16, 'chr2:-:154585759:TE': 0}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': 64, 'chr14:+:54369669:TE': 4}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': 49, 'chr3:-:129982765:TE': 1}}\n",
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': 3, 'chr11:+:55113026:TE': 1}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': 212, 'chr2:+:181515774:TE': 102, 'chr2:+:181516756:TE': 26}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': 16, 'chr2:-:154585759:TE': 0}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': 64, 'chr14:+:54369669:TE': 4}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': 49, 'chr3:-:129982765:TE': 1}}\n",
      "5\n",
      "5\n",
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': 3, 'chr11:+:55113026:TE': 1}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': 212, 'chr2:+:181515774:TE': 102, 'chr2:+:181516756:TE': 26}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': 16, 'chr2:-:154585759:TE': 0}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': 64, 'chr14:+:54369669:TE': 4}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': 49, 'chr3:-:129982765:TE': 1}}\n",
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': 2, 'chr11:+:55113026:TE': 0}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': 268, 'chr2:+:181515774:TE': 90, 'chr2:+:181516756:TE': 27}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': 20, 'chr2:-:154585759:TE': 0}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': 66, 'chr14:+:54369669:TE': 15}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': 51, 'chr3:-:129982765:TE': 0}}\n"
     ]
    }
   ],
   "source": [
    "#WT 1\n",
    "\n",
    "wt_1 = pysam.AlignmentFile(m323k_wt_1_bam_path, \"rb\")\n",
    "\n",
    "\n",
    "jnc_counts_dict = {}\n",
    "\n",
    "for transcript, polya_dict in small_polya_jnc_coords.items():\n",
    "    # store junction read counts for every poly(A) site in transcript\n",
    "    tr_polya_dict = {}\n",
    "    #print(polya_dict)\n",
    "    \n",
    "    for polya_site, coords_tuple in polya_dict.items():\n",
    "        \n",
    "        #Reads overlapping poly(A) site\n",
    "        site_jnc_read_count = 0\n",
    "        \n",
    "        for read_entry in wt_1.fetch(coords_tuple[0], coords_tuple[1], coords_tuple[2]):\n",
    "            \n",
    "            #0-based leftmost reference coordinate of the aligned sequence\n",
    "            align_start = read_entry.reference_start\n",
    "            \n",
    "            # Aligned reference position of the read on the reference genome.\n",
    "            # Reference_end points to one past the last aligned residue. Returns None if not available (read is unmapped or no cigar alignment present).\n",
    "\n",
    "            align_end = read_entry.reference_end\n",
    "            \n",
    "            #print(read_entry.query_name)\n",
    "            #print(\"align_start: {0}, align_end: {1}\".format(str(align_start), str(align_end)))\n",
    "            \n",
    "            #polyA site - align_start & align_end - polya_site\n",
    "            if abs(coords_tuple[1] - align_start) > 3 and abs(align_end - coords_tuple[1]) > 3:\n",
    "                site_jnc_read_count += 1\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #Now have checked every read overlapping poly(A) site\n",
    "        tr_polya_dict[polya_site] = site_jnc_read_count\n",
    "        \n",
    "    \n",
    "    #Now have counted junction reads for each poly(A) site in transcript\n",
    "    jnc_counts_dict[transcript] = tr_polya_dict\n",
    "    \n",
    "    \n",
    "\n",
    "#print(jnc_counts_dict)\n",
    "\n",
    "\n",
    "def is_polya_junction_read(read_entry,polya_start, sj_overhang):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #0-based leftmost reference coordinate of the aligned sequence\n",
    "    align_start = read_entry.reference_start\n",
    "           \n",
    "    # Aligned reference position of the read on the reference genome.\n",
    "    # Reference_end points to one past the last aligned residue. Returns None if not available (read is unmapped or no cigar alignment present).\n",
    "\n",
    "    align_end = read_entry.reference_end\n",
    "    \n",
    "    if abs(polya_start - align_start) > sj_overhang and abs(align_end - polya_start) > sj_overhang:\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "    \n",
    "#Try a more compartmentalised version of count_dict\n",
    "jnc_counts_dict_2 = {}\n",
    "\n",
    "for transcript, polya_dict in small_polya_jnc_coords.items():\n",
    "    \n",
    "    tr_polya_counts = {polya_site: sum(is_polya_junction_read(read, coords_tuple[1], 3) for read in wt_1.fetch(coords_tuple[0], \n",
    "                                                                            coords_tuple[1], \n",
    "                                                                            coords_tuple[2]))\n",
    "                       for polya_site, coords_tuple in polya_dict.items()}\n",
    "\n",
    "    jnc_counts_dict_2[transcript] = tr_polya_counts\n",
    "\n",
    "    \n",
    "#print(jnc_counts_dict_2)\n",
    "\n",
    "wt_1.close()\n",
    "\n",
    "def get_polya_junction_counts_dict(bam_path, jnc_coords_dict, sj_overhang):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    bam = pysam.AlignmentFile(bam_path, \"rb\")\n",
    "    \n",
    "    #final output dict of {tr: {site_id: jnc_counts}}\n",
    "    counts_dict = {}\n",
    "    \n",
    "    for transcript, polya_coords_dict in jnc_coords_dict.items():\n",
    "        \n",
    "        polya_coords_counts = {polya_site: sum(is_polya_junction_read(read,\n",
    "                                                                      coords_tuple[1],\n",
    "                                                                      sj_overhang) \n",
    "                                               \n",
    "                                               for read in bam.fetch(coords_tuple[0], \n",
    "                                                                     coords_tuple[1], \n",
    "                                                                     coords_tuple[2])\n",
    "                                              )\n",
    "                               for polya_site, coords_tuple in polya_coords_dict.items()}\n",
    "        \n",
    "        counts_dict[transcript] = polya_coords_counts\n",
    "        \n",
    "    \n",
    "    bam.close()\n",
    "    \n",
    "    return counts_dict\n",
    "\n",
    "wt_1_polya_jnc_counts = get_polya_junction_counts_dict(m323k_wt_1_bam_path, small_polya_jnc_coords, 3)\n",
    "hom_1_polya_jnc_counts = get_polya_junction_counts_dict(m323k_hom_1_bam_path, small_polya_jnc_coords, 3)\n",
    "\n",
    "#print(len(wt_1_polya_jnc_counts))\n",
    "#print(len(hom_1_polya_jnc_counts))\n",
    "\n",
    "print(wt_1_polya_jnc_counts)\n",
    "print(hom_1_polya_jnc_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now I need counts for junction reads at 5' end of terminal exon**\n",
    "\n",
    "The coordinates are provided in the paqr output table, - suggest pulling from the exon_id string\n",
    "\n",
    "Coordinates are bed-like i.e. 0-based, 1/2 open - start is included, end is not\n",
    "I want the splice junction coordinates to follow this\n",
    "if + strand then start of exon = start coordinate\n",
    "if - strand then start of exon = end coordinate (-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENSMUST00000000608': ('chr11', 55109377, 55109378), 'ENSMUST00000000844': ('chr2', 181515079, 181515080), 'ENSMUST00000000896': ('chr2', 154588091, 154588092), 'ENSMUST00000000985': ('chr14', 54368317, 54368318), 'ENSMUST00000001079': ('chr3', 129984005, 129984006)}\n",
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': ('chr11', 55110898, 55110899), 'chr11:+:55113026:TE': ('chr11', 55113026, 55113027)}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': ('chr2', 181515384, 181515385), 'chr2:+:181515774:TE': ('chr2', 181515774, 181515775), 'chr2:+:181516756:TE': ('chr2', 181516756, 181516757)}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': ('chr2', 154587124, 154587125), 'chr2:-:154585759:TE': ('chr2', 154585759, 154585760)}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': ('chr14', 54368610, 54368611), 'chr14:+:54369669:TE': ('chr14', 54369669, 54369670)}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': ('chr3', 129983184, 129983185), 'chr3:-:129982765:TE': ('chr3', 129982765, 129982766)}}\n"
     ]
    }
   ],
   "source": [
    "#Want dictionary of {transcript_id: (chr, start, end, )}\n",
    "\n",
    "#print(small_polya_jnc_coords)\n",
    "#print(rel_usages)\n",
    "\n",
    "def paqr_out_to_sj_coord_tuple(paqr_df, \n",
    "                               group_col = \"gene_id\",\n",
    "                               exon_colname = \"paqr_exon_id\",\n",
    "                               chr_colname = \"chr\", \n",
    "                               strand_colname = \"strand\"):\n",
    "    '''\n",
    "    Get dict of {transcript_id: (chr, start, end)} where coords are for 5' splice junction of terminal exon\n",
    "    '''\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    df_grouped = paqr_df.groupby(group_col)\n",
    "    \n",
    "    for group_name, group in df_grouped:\n",
    "        \n",
    "        \n",
    "        # Every id should have same strand, chromosome and terminal exon string, so only need 1 row\n",
    "        # List selection ensures return a dataframe\n",
    "        group = group.iloc[[0]]\n",
    "        \n",
    "        #exon id like ENSMUST00000007216:12:12:20543310:20544909\n",
    "        exon_split = group[exon_colname].to_string(index = False).split(':')\n",
    "        \n",
    "        if (group[strand_colname] == \"+\").bool():\n",
    "            #5 coord = start of te (& start coord = actual start)\n",
    "                        \n",
    "            coord_tuple = tuple([group[chr_colname].to_string(index = False).lstrip(' '),\n",
    "                                int(exon_split[-2]),\n",
    "                                int(exon_split[-2]) + 1\n",
    "                                ])\n",
    "            \n",
    "        elif (group[strand_colname] == \"-\").bool():\n",
    "            #5 of exon = end coord in string (half-open, so actual start coord = end -1)\n",
    "            \n",
    "            coord_tuple = tuple([group[chr_colname].to_string(index = False).lstrip(' '),\n",
    "                                int(exon_split[-1]) -1,\n",
    "                                int(exon_split[-1])\n",
    "                                ])\n",
    "            \n",
    "        out_dict[group_name] = coord_tuple\n",
    "    \n",
    "    return out_dict\n",
    "\n",
    "\n",
    "splice_jnc_coords = paqr_out_to_sj_coord_tuple(rel_usages)\n",
    "#print(splice_jnc_coords)\n",
    "\n",
    "\n",
    "#Now lets make a small_splice_jnc_coords with same sjs as in small_polya_jnc_coords\n",
    "\n",
    "small_splice_jnc_coords = {key: val for key, val in splice_jnc_coords.items() \n",
    "                           if key in small_polya_jnc_coords.keys()}\n",
    "\n",
    "print(small_splice_jnc_coords)\n",
    "print(small_polya_jnc_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENSMUST00000000608': 310, 'ENSMUST00000000844': 450, 'ENSMUST00000000896': 30, 'ENSMUST00000000985': 183, 'ENSMUST00000001079': 192}\n",
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': 3, 'chr11:+:55113026:TE': 1}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': 212, 'chr2:+:181515774:TE': 102, 'chr2:+:181516756:TE': 26}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': 16, 'chr2:-:154585759:TE': 0}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': 64, 'chr14:+:54369669:TE': 4}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': 49, 'chr3:-:129982765:TE': 1}}\n",
      "{'ENSMUST00000000608': 475, 'ENSMUST00000000844': 482, 'ENSMUST00000000896': 48, 'ENSMUST00000000985': 244, 'ENSMUST00000001079': 196}\n",
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': 2, 'chr11:+:55113026:TE': 0}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': 268, 'chr2:+:181515774:TE': 90, 'chr2:+:181516756:TE': 27}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': 20, 'chr2:-:154585759:TE': 0}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': 66, 'chr14:+:54369669:TE': 15}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': 51, 'chr3:-:129982765:TE': 0}}\n"
     ]
    }
   ],
   "source": [
    "## Ok now need to search bam for reads overlapping splice jnc coord,\n",
    "## count number of aligned reads crossing splice junction\n",
    "## \n",
    "\n",
    "def get_splice_junction_counts_dict_depr(bam_path, jnc_coords_dict):\n",
    "    '''\n",
    "    output dict of {tr: count}\n",
    "    '''\n",
    "\n",
    "    bam = pysam.AlignmentFile(bam_path, \"rb\")\n",
    "    \n",
    "    #final output dict of {tr: {site_id: jnc_counts}}\n",
    "    counts_dict = {transcript: (sum(1 for read in bam.fetch(coords_tuple[0], \n",
    "                                              coords_tuple[1], \n",
    "                                              coords_tuple[2])))\n",
    "                  for transcript, coords_tuple in jnc_coords_dict.items()}\n",
    "    \n",
    "    bam.close()\n",
    "    \n",
    "    return counts_dict\n",
    "\n",
    "wt_1_splice_jnc_counts_depr = get_splice_junction_counts_dict_depr(m323k_wt_1_bam_path, small_splice_jnc_coords)\n",
    "hom_1_splice_jnc_counts_depr = get_splice_junction_counts_dict_depr(m323k_hom_1_bam_path, small_splice_jnc_coords)\n",
    "\n",
    "print(wt_1_splice_jnc_counts_depr)\n",
    "print(wt_1_polya_jnc_counts)\n",
    "\n",
    "print(hom_1_splice_jnc_counts_depr)\n",
    "print(hom_1_polya_jnc_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These counts are way way off... (NB I don't think this is necessarily the case - remember reads overlapping poly(A) sites = support for downstream poly(A) sites - these transcripts could just have really high usage of proximal site. Point still stands though, I need to be much smarter)\n",
    "\n",
    "I think I'm going to actually have to do some smart filtering to get junction read counts\n",
    "\n",
    "i.e. for every read in bam.fetch(splice junction), check read is\n",
    "primary alignment/uniquely aligned\n",
    "\n",
    "is a split alignment i.e has 'N' in cigar string & this split alignment starts at the splice junction i.e. read skips a bit of reference sequence (intron) then starts at splice junction with contiguous/consective alignment to reference (i.e. exon)\n",
    "\n",
    "`~~~` - intron\n",
    "\n",
    "`___ `- exon\n",
    "\n",
    "`|` - splice junction\n",
    "\n",
    "`------` - read\n",
    "\n",
    "I'm interested in reads spanning  the splice junction of 'exon 2' (i.e. the TE)\n",
    "\n",
    "What i'm really after is the total number of reads supporting inclusion of/crossing this junction\n",
    "I'm not particularly interested which upstream exon/ splice junction it is spliced to, just that it supports inclusion of the terminal exon junction\n",
    "\n",
    "If I want to make sure it is a read connecting two exons (but without caring what the other junction is), I can check the cigar string such that the intron (N) is bounded by exact reference matches of at least (3) nt on either side e.g. a string like\n",
    "**70M84N5M** or for readability **70M 84N 5M** \n",
    "would be a valid read because a reference gap of 84N (intron) is separated by a consecutive match to reference (exon) >= 3nt on either side\n",
    "\n",
    "To be extra sure, I should really check the N & M segments align with reference coordinate/junction I'm interested in...\n",
    "\n",
    "**to this end, I just want reads that cross the TE splice junction, and contain a 'right hand overhang'. --alignSJDBoverhangMin, of at least x nt (default 3nt) i.e. x nt of the terminal exon** \n",
    "\n",
    "Maybe also only want to count spliced alignments, but for now let's not worry about that??\n",
    "\n",
    "\n",
    "5' exon 1                            exon 2            3'\n",
    "\n",
    "`____________|~~~~~~~~~~~~~~~~~~|______________`\n",
    "\n",
    "`xxxxxx-------NNNNNNNNNNNNNNNNNN-------xxxxxxxx` Read supporting splicing-in/inclusion of this TE\n",
    "\n",
    "`xx-----------NNNNNNNNNNNNNNNNNN--xxxxxxxxxxxxx`\n",
    "\n",
    "`xxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------xx`\n",
    "\n",
    "\n",
    "check if right hand overhang is > --alignSJDBoverhangMin (usually 3nt) i.e. aligned portion of read has at least 3nt of terminal exon sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENSMUST00000000608': (('chr11', 55109377, 55109378), 305), 'ENSMUST00000000844': (('chr2', 181515079, 181515080), 432), 'ENSMUST00000000896': (('chr2', 154588091, 154588092), 29), 'ENSMUST00000000985': (('chr14', 54368317, 54368318), 167), 'ENSMUST00000001079': (('chr3', 129984005, 129984006), 169)}\n",
      "{'ENSMUST00000000608': (('chr11', 55109377, 55109378), 468), 'ENSMUST00000000844': (('chr2', 181515079, 181515080), 462), 'ENSMUST00000000896': (('chr2', 154588091, 154588092), 42), 'ENSMUST00000000985': (('chr14', 54368317, 54368318), 227), 'ENSMUST00000001079': (('chr3', 129984005, 129984006), 175)}\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read has minimum overhang & is valid junction read\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "read does not have minimum exon-exon overhang\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "Read does not have a putative exon-exon junction alignment\n",
      "158\n",
      "('chr14', 54368317, 54368318)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "test_tr_splice_jnc_tuple = small_splice_jnc_coords.get('ENSMUST00000000985')\n",
    "#print(test_tr_splice_jnc_tuple)\n",
    "\n",
    "\n",
    "\n",
    "x = 0\n",
    "\n",
    "\n",
    "#test_reads_dict = {}\n",
    "#for read in wt_1.fetch(test_tr_splice_jnc_tuple[0],\n",
    "#                    test_tr_splice_jnc_tuple[1],\n",
    "#                    test_tr_splice_jnc_tuple[2]):\n",
    "    \n",
    "#    while x < 10:\n",
    "#        if test_tr_splice_jnc_tuple in test_reads_dict.keys():\n",
    "#            #add AlignedSegment object for read to end of list for given splice jnc\n",
    "#            test_reads_dict[test_tr_splice_jnc_tuple].append(read.to_dict())\n",
    "#        \n",
    "#        else:\n",
    "#            test_reads_dict[test_tr_splice_jnc_tuple] = [read.to_dict()]\n",
    "#            \n",
    "#        x += 1\n",
    "#    else:\n",
    "#        break\n",
    "    \n",
    "#print(x)\n",
    "#print(test_reads_dict)\n",
    "\n",
    "'''\n",
    "cigartuples\n",
    "\n",
    "    the cigar alignment. The alignment is returned as a list of tuples of (operation, length).\n",
    "\n",
    "    If the alignment is not present, None is returned.\n",
    "\n",
    "    The operations are:\n",
    "    M \tBAM_CMATCH \t0\n",
    "    I \tBAM_CINS \t1\n",
    "    D \tBAM_CDEL \t2\n",
    "    N \tBAM_CREF_SKIP \t3\n",
    "    S \tBAM_CSOFT_CLIP \t4\n",
    "    H \tBAM_CHARD_CLIP \t5\n",
    "    P \tBAM_CPAD \t6\n",
    "    = \tBAM_CEQUAL \t7\n",
    "    X \tBAM_CDIFF \t8\n",
    "    B \tBAM_CBACK \t9\n",
    "\n",
    "\n",
    "I neeed to check for M followed by \n",
    "\n",
    "'''\n",
    "\n",
    "#spliced_read_test_count_dict = {}\n",
    "\n",
    "\n",
    "def get_splice_junction_counts_dict(bam_path, jnc_coords_dict, sj_overhang):\n",
    "    '''\n",
    "    Return dict of {tr: (sj_coord_tuple, jnc_count)}\n",
    "    '''\n",
    "    \n",
    "    bam = pysam.AlignmentFile(bam_path, \"rb\")\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    # Want to get indexes of Ms in a M | N | M sequence from cigartuples\n",
    "    # (can then slice parsed_cigar to check whether Ms are long enough)\n",
    "    # M | N | M = [0, 3, 0] (in cigartuples looking to match to first element in each tuple)\n",
    "    \n",
    "    seq_to_check = [0, 3, 0]\n",
    "    \n",
    "    for transcript, coords_tuple in jnc_coords_dict.items():\n",
    "        \n",
    "        jnc_count = 0\n",
    "        for read in bam.fetch(coords_tuple[0],\n",
    "                              coords_tuple[1],\n",
    "                              coords_tuple[2]):\n",
    "            \n",
    "            # check for match (M) | reference skip/intron (N) | match (M)\n",
    "            parsed_cigar = read.cigartuples\n",
    "            \n",
    "            # Want to get indexes of Ms in a M | N | M sequence from cigartuples\n",
    "            # If want to check intron/M start for consistency with SJ, remove the -1 \n",
    "            cigar_check = [(i, i + len(seq_to_check) - 1) for i in range(len(parsed_cigar))\n",
    "                       if [tup[0] for tup in parsed_cigar[i:i + len(seq_to_check)]] == seq_to_check]\n",
    "            \n",
    "            \n",
    "            if len(cigar_check) > 0:\n",
    "                \n",
    "                #Do Ms in M | N | M sequence have minimum sj_overhang for both Ms?\n",
    "                cigar_valid = [True for idx_start, idx_end in cigar_check\n",
    "                               if parsed_cigar[idx_start][1] >= sj_overhang \n",
    "                               and parsed_cigar[idx_end][1] >= sj_overhang]\n",
    "                \n",
    "                if len(cigar_valid) > 0:\n",
    "                    # Read has valid alignment spanning junction\n",
    "                    jnc_count += 1\n",
    "                \n",
    "                else:\n",
    "                    #Read spans a exon-exon junction \n",
    "                    #but doesn't have enough exonic alignment either side to count read\n",
    "                    continue\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                continue\n",
    "        \n",
    "        #Now have checked every read with alignment at sj position\n",
    "        out_dict[transcript] = tuple([coords_tuple, jnc_count])\n",
    "        \n",
    "    bam.close()\n",
    "    \n",
    "    return out_dict\n",
    "\n",
    "\n",
    "\n",
    "wt_1_splice_jnc_counts_small = get_splice_junction_counts_dict(m323k_wt_1_bam_path, small_splice_jnc_coords, 3)\n",
    "hom_1_splice_jnc_counts_small = get_splice_junction_counts_dict(m323k_hom_1_bam_path, small_splice_jnc_coords, 3)\n",
    "\n",
    "print(wt_1_splice_jnc_counts_small)\n",
    "print(hom_1_splice_jnc_counts_small)\n",
    "\n",
    "'''\n",
    "wt_1 = pysam.AlignmentFile(m323k_wt_1_bam_path, \"rb\")\n",
    "jnc_count = 0\n",
    "for read in wt_1.fetch(test_tr_splice_jnc_tuple[0],\n",
    "                    test_tr_splice_jnc_tuple[1],\n",
    "                    test_tr_splice_jnc_tuple[2]):\n",
    "    \n",
    "    # check for match (M) | reference skip/intron (N) | match (M)\n",
    "    parsed_cigar = read.cigartuples\n",
    "    #print(parsed_cigar)\n",
    "    #print(len(parsed_cigar))\n",
    "        \n",
    "    # Want to get indexes of Ms in a M | N | M sequence fro cigartuples\n",
    "    # (can then slice parsed_cigar to check whether Ms are long enough)\n",
    "    # M | N | M = [0, 3, 0] (in cigartuples looking to match to first element in each tuple)\n",
    "    seq_to_check = [0, 3, 0]\n",
    "        \n",
    "    #print(parsed_cigar[0:0+len(seq_to_check)])\n",
    "        \n",
    "        \n",
    "    cigar_check = [(i, i + len(seq_to_check) - 1) for i in range(len(parsed_cigar))\n",
    "                       if [tup[0] for tup in parsed_cigar[i:i + len(seq_to_check)]] == seq_to_check]\n",
    "        \n",
    "    #print(len(cigar_check))\n",
    "        \n",
    "    if len(cigar_check) > 0:\n",
    "        # Has a putative exon - exon alignment\n",
    "        # Now check if length of matched sequences is sufficient\n",
    "        \n",
    "        for idx_start, idx_end in cigar_check:\n",
    "                \n",
    "            # Now check each string in matching sequence\n",
    "            if parsed_cigar[idx_start][1] > 3 and parsed_cigar[idx_end][1] > 3:\n",
    "                print(\"read has minimum overhang & is valid junction read\")\n",
    "                jnc_count += 1\n",
    "\n",
    "            else:\n",
    "                print(\"read does not have minimum exon-exon overhang\")\n",
    "                        \n",
    "                    \n",
    "    else:\n",
    "        print(\"Read does not have a putative exon-exon junction alignment\")            \n",
    "       \n",
    "        \n",
    "print(jnc_count)    \n",
    "\n",
    "print(test_tr_splice_jnc_tuple)\n",
    "#for col in wt_1.pileup(test_tr_splice_jnc_tuple[0],\n",
    "#                      test_tr_splice_jnc_tuple[1],\n",
    "#                      test_tr_splice_jnc_tuple[2]):\n",
    "#    print(\"coverage at position {0} : {1}\".format(col.pos, col.n))\n",
    "    \n",
    "count = 0\n",
    "for col in wt_1.fetch('chr2', 154588091, 154588092):\n",
    "    count +=1\n",
    "\n",
    "print(count)\n",
    "wt_1.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I've got a splice jnc counting function (I should make a helper to map the polya counts and sj counts dicts...)\n",
    "\n",
    "\n",
    "**Does the SJ / poly(A) junction count generate similar poly(A) site relative usage values to PAQR?**\n",
    "\n",
    "(this makes or breaks my idea I guess...)\n",
    "\n",
    "Basically, my thought is if relative junction counts can 'accurately' quantify poly(A) site relative usage, then this can be extended to incorporate other types of APA (where TE may not necessarily have multiple poly(A) sites) & capture all possible APA types under the same metric\n",
    "\n",
    "e.g. intronic poly(A) sites (if define exon using tool like TECTool)\n",
    "e.g. ALE isoforms \n",
    "\n",
    "**If TE has a single poly(A) site/gene has multiple TEs, then can represent usage at the TE level by the number of spliced reads supporting the inclusion of its splice junction**\n",
    "\n",
    "**If TE has multiple poly(A) sites, usage at the poly(A) site level can be defined by the number of reads spanning the poly(A) site relative to the number of splice-junction reads. In this case, the number of reads crossing the poly(A) site provides evidence for usage/support of all downstream sites**\n",
    "\n",
    "Usage at poly(A) site = (upstream jnc counts - site jnc counts) / total counts at SJ\n",
    "\n",
    "Note site jnc counts i.e. number of 'jnc reads' at poly(A) sites effectively quantifies n reads supporting usage of all *downstream sites*\n",
    "\n",
    "\n",
    "`SJ                pA1              pA2                  pA3`\n",
    "\n",
    "`|__________________!_______________!_____________________!`\n",
    "\n",
    "`10                 4               2                     0` Jnc counts (pA3 is not necessarily 0, but expect it to be close)\n",
    "\n",
    "`NA             (10-4)/10       (4-2)/10                 (2 - 0)/10` poly(A) site usage formula\n",
    "\n",
    "`NA               0.6/ 60%        0.2/ 20%               0.2 / 20%`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+']\n",
      "{55110898: 'chr11:+:55110898:TE', 55113026: 'chr11:+:55113026:TE'}\n",
      "[55110898, 55113026]\n",
      "['+']\n",
      "{181515384: 'chr2:+:181515384:TE', 181515774: 'chr2:+:181515774:TE', 181516756: 'chr2:+:181516756:TE'}\n",
      "[181515384, 181515774, 181516756]\n",
      "['-']\n",
      "{154587124: 'chr2:-:154587124:TE', 154585759: 'chr2:-:154585759:TE'}\n",
      "[154587124, 154585759]\n",
      "['+']\n",
      "{54368610: 'chr14:+:54368610:TE', 54369669: 'chr14:+:54369669:TE'}\n",
      "[54368610, 54369669]\n",
      "['-']\n",
      "{129983184: 'chr3:-:129983184:TE', 129982765: 'chr3:-:129982765:TE'}\n",
      "[129983184, 129982765]\n",
      "{'ENSMUST00000000608': {'chr11:+:55110898:TE': 0.9901639344262295, 'chr11:+:55113026:TE': 0.009836065573770493}, 'ENSMUST00000000844': {'chr2:+:181515384:TE': 0.5092592592592593, 'chr2:+:181515774:TE': 0.25462962962962965, 'chr2:+:181516756:TE': 0.2361111111111111}, 'ENSMUST00000000896': {'chr2:-:154587124:TE': 0.4482758620689655, 'chr2:-:154585759:TE': 0.5517241379310345}, 'ENSMUST00000000985': {'chr14:+:54368610:TE': 0.6167664670658682, 'chr14:+:54369669:TE': 0.38323353293413176}, 'ENSMUST00000001079': {'chr3:-:129983184:TE': 0.7100591715976331, 'chr3:-:129982765:TE': 0.28994082840236685}}\n"
     ]
    }
   ],
   "source": [
    "def calc_jnc_polya_relative_usage(polya_jnc_counts_dict, splice_jnc_counts_dict):\n",
    "    '''\n",
    "    Calculate relative usage of poly(A) site using junction counts\n",
    "    Formula for each poly(A) site \n",
    "    = (upstream jnc counts - site jnc counts) / total counts at SJ\n",
    "    \n",
    "    1. Sort poly(A) sites 5'-3' in strand aware manner\n",
    "    2. Calculate relative usage for each site, starting at most proximal --> most distal\n",
    "    3. output to dict\n",
    "    \n",
    "    Note that the jnc count for the most 3' site will be arbitrarily set to 0 for now, whilst I think of better way to handle\n",
    "    For purpose of comparing to PAQR this should be fine, but note that a > 0 is not unexpected.\n",
    "    Could be noisy/consequence of cleavage imprecision for same site, or evidence that downstream processing site is being used\n",
    "    For purposes of comparing to PAQR, this should be adequate, but may introduce some bias/inaccuracy...\n",
    "    \n",
    "    \n",
    "    :param polya_jnc_counts_dict: dict\n",
    "        dictionary of {transcript: {polyA_site_id: jnc_count }} generated by get_polya_junction_counts_dict\n",
    "        \n",
    "    :param splice_jnc_counts_dict: dict\n",
    "        dictionary of {transcript: (splice_jnc_tuple, jnc_count)} generated by get_splice_junction_counts_dict\n",
    "    \n",
    "    \n",
    "    :return rel_usage_dict: dict\n",
    "        dict of {transcript: {pA1: <relative_usage>, pA2: <relative_usage>, ..., pAn: <relative_usage}}\n",
    "    '''\n",
    "    \n",
    "    rel_usage_dict = {}\n",
    "    \n",
    "    for transcript, polya_counts_dict in polya_jnc_counts_dict.items():\n",
    "        \n",
    "        #1. get list of poly(A) site ids & sort 5'-3' according to strand\n",
    "        polya_site_ids = list(polya_counts_dict.keys())\n",
    "        \n",
    "        # site id formatted like chr11:+:55110898:TE\n",
    "        strand = set([id.split(':')[1] for id in polya_site_ids])\n",
    "        coord_dict = {id: int(id.split(':')[2]) for id in polya_site_ids}\n",
    "        \n",
    "        print(strand)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return rel_usage_dict\n",
    "\n",
    "temp_rel_use_dict = {}\n",
    "for transcript, polya_counts_dict in wt_1_polya_jnc_counts.items():\n",
    "        \n",
    "        #1. get list of poly(A) site ids & sort 5'-3' according to strand\n",
    "        polya_site_ids = list(polya_counts_dict.keys())\n",
    "        \n",
    "        # site id formatted like chr11:+:55110898:TE\n",
    "        strand = list(set([id.split(':')[1] for id in polya_site_ids]))\n",
    "        coord_dict = {int(id.split(':')[2]): id for id in polya_site_ids}\n",
    "        \n",
    "        if len(strand) > 1:\n",
    "            print(\"Poly(A) sites assigned to transcript are not all on same strand. Skipping transcript {0}\".format(transcript))\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        print(strand)\n",
    "        print(coord_dict)\n",
    "        \n",
    "        if strand[0] == \"+\":\n",
    "            #Sort coords 5' - 3' (smallest to largest/ascending order)\n",
    "            sorted_coords = sorted(list(coord_dict.keys()))\n",
    "            \n",
    "        elif strand[0] == \"-\":\n",
    "            #Sort descending order to get 5'-3'\n",
    "            sorted_coords = sorted(list(coord_dict.keys()), reverse=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"poly(A) sites have invalid strand string. Skipping transcript {0}\".format(transcript))\n",
    "            continue\n",
    "        \n",
    "        print(sorted_coords)\n",
    "        \n",
    "        #2. Quantify relative usage of each site in transcript\n",
    "        \n",
    "        sj_count = wt_1_splice_jnc_counts_small.get(transcript)[1] # {tr : (sj_coords_tuple, jnc_count)}\n",
    "        polya_usage_dict = {} # store {polya_site_id: rel_usage} for this transcript\n",
    "        \n",
    "        for site_idx, site_coord in enumerate(sorted_coords):\n",
    "            \n",
    "            if site_idx == 0:\n",
    "                #Most 5' site in tr - (sj - polya_jnc_count/ sj)\n",
    "                upstream_count = sj_count\n",
    "                downstream_count = polya_counts_dict.get(coord_dict.get(site_coord))\n",
    "                site_rel_use = (upstream_count - downstream_count) / sj_count\n",
    "            \n",
    "            elif site_idx == (len(sorted_coords) - 1):\n",
    "                #Most 3' site in tr - jnc count considered 0 for now...\n",
    "                #((site_idx - 1)_polya_counts - 0) / sj)\n",
    "                upstream_count = polya_counts_dict.get(coord_dict.get(sorted_coords[(site_idx - 1)]))\n",
    "                downstream_count = 0          \n",
    "                site_rel_use = (upstream_count - downstream_count) / sj_count\n",
    "\n",
    "            else:\n",
    "                # Site is intermediate in transcript/TE\n",
    "                # (site_idx - 1)_polya_counts - site_idx_counts)\n",
    "                \n",
    "                upstream_count = polya_counts_dict.get(coord_dict.get(sorted_coords[(site_idx - 1)]))\n",
    "                downstream_count = polya_counts_dict.get(coord_dict.get(site_coord))\n",
    "                site_rel_use = (upstream_count - downstream_count) / sj_count\n",
    "                \n",
    "            #site id: rel_usage\n",
    "            polya_usage_dict[coord_dict.get(site_coord)] = site_rel_use\n",
    "            \n",
    "            \n",
    "        # Every site in transcript quantified...\n",
    "        temp_rel_use_dict[transcript] = polya_usage_dict\n",
    "                \n",
    "print(temp_rel_use_dict)            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot relative usage dict to a pandas dataframe\n",
    "Assign site_along_exon (essentially 5'-3' order on transcript in strand aware manner)\n",
    "Join with paqr relative usage calculations on transcript, site along exon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:polya_junctions] *",
   "language": "python",
   "name": "conda-env-polya_junctions-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
