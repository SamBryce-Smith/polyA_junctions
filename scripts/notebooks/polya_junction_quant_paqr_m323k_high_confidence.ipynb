{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying poly(A) sites as junction counts relative to 5' TE splice junction\n",
    "\n",
    "\n",
    "The throw it out there idea is that if this is reliable (i.e. just considering counts across poly(A) sites (accounting for minimum overhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pysam\n",
    "os.getcwd()\n",
    "\n",
    "paqr_rel_usages_path = \"../../data/relative_usages.filtered.tsv\"\n",
    "rel_usages_header_path = \"../../data/relative_usages.header.out\"\n",
    "m323k_wt_1_bam_path = \"/home/sam/cluster/TDP43_RNA/TDP_F210I_M323K/M323K/New_adult_brain/processed/M323K_WT_1/M323K_WT_1_unique_rg_fixed.bam\"\n",
    "m323k_hom_1_bam_path = \"/home/sam/cluster/TDP43_RNA/TDP_F210I_M323K/M323K/New_adult_brain/processed/M323K_HOM_1/M323K_HOM_1_unique_rg_fixed.bam\"\n",
    "\n",
    "rel_usages = pd.read_csv(paqr_rel_usages_path,sep=\"\\t\")\n",
    "\n",
    "with open(rel_usages_header_path) as inpt:\n",
    "    \n",
    "    sample_names = [line.rstrip() for line in inpt]\n",
    "\n",
    "\n",
    "#print(rel_usages)\n",
    "#print(sample_names)\n",
    "\n",
    "#First 10 columns are the same in relative usages df\n",
    "colnames_rel_usages = [\"chr\",\n",
    " \"cluster_start\",\n",
    " \"cluster_end\",\n",
    " \"site_id\",\n",
    " \"score\",\n",
    " \"strand\",\n",
    " \"n_along_exon\",\n",
    " \"total_sites_on_exon\", \n",
    " \"paqr_exon_id\", \n",
    " \"gene_id\" # this is technically transcript_id but for now I'll leave it...\n",
    "]\n",
    "\n",
    "#Rest of columns are samples names in order found in relative usages output df\n",
    "colnames_rel_usages.extend(sample_names)\n",
    "\n",
    "print(colnames_rel_usages)\n",
    "\n",
    "\n",
    "#Because samples are paired according to config - can appear multiple times in df...\n",
    "sample_names = set(sample_names)\n",
    "print(sample_names)\n",
    "\n",
    "rel_usages.columns = colnames_rel_usages\n",
    "print(rel_usages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#grouped = rel_usages.groupby(\"gene_id\")\n",
    "#print(grouped)\n",
    "\n",
    "#for a, b in grouped:\n",
    "#    print(a)\n",
    "#    print(b[\"site_id\"].to_list())\n",
    "\n",
    "    \n",
    "    \n",
    "def polyASite_id_to_coordinate_tuple(paqr_df, site_id_colname = \"site_id\", group_col = \"gene_id\"):\n",
    "    '''\n",
    "    Nested dict of {<group_col_key>: {site_id: (chr, start, end)}}}\n",
    "    Assume coord is 1 based, output will be 0 based, 1/2 open\n",
    "    '''\n",
    "    \n",
    "    df_grouped = paqr_df.groupby(group_col)\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    for group_name, group in df_grouped:\n",
    "        \n",
    "        site_ids = group[site_id_colname].to_list()\n",
    "        nested_dict = {}\n",
    "        \n",
    "        for site in site_ids:\n",
    "            #'chr11:+:55110898:TE'\n",
    "            ID = site.split(':')\n",
    "            seq_tuple = (ID[0], int(ID[2]), int(ID[2]) + 1)\n",
    "            \n",
    "            nested_dict[site] = seq_tuple \n",
    "            \n",
    "        out_dict[group_name] = nested_dict\n",
    "        \n",
    "        \n",
    "    return out_dict\n",
    "\n",
    "\n",
    "polya_jnc_coords = polyASite_id_to_coordinate_tuple(rel_usages)\n",
    "#print(polya_jnc_coords)\n",
    "\n",
    "\n",
    "### For testing, let's just look at a few transcripts\n",
    "small_polya_jnc_coords = {key: polya_jnc_coords[key] for key in list(polya_jnc_coords.keys())[:5]}\n",
    "#print(small_polya_jnc_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I've got my PAQR inferred/ PolyASite poly(A) sites , I want to try and count the number of reads that span these positions - 'junction reads'\n",
    "\n",
    "As I will want to quantify these junctions relative to the splice junction at the 5'end of the terminal exon, I have to treat the poly(A) junction alignments as if they were a splice junction (where sequence is disjointly aligned to the genome\n",
    "\n",
    "STAR (and likely other splice-aware aligners) has a --alignSJDBoverhangMin parameter, which requires that a putative junction read 'overhangs' the junction by at least x nt (x = 3 by default) in orfer for it to be assigned to the junction\n",
    "\n",
    "\n",
    "Reads aligned to genome at poly(A) sites aren't subject to this parameter, so junction counts would be inflated relative to the 5' splice junction\n",
    "\n",
    "\n",
    "using pysam, for each read crossing the poly(A) site, check whether start or end of read alignment falls within 3nt of the poly(A) site coordinate\n",
    "\n",
    "abs(polya_site - alignment_start) > 3 and abs(alignment_end - polya_site) > 3 = valid poly(A) site junction read\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WT 1\n",
    "\n",
    "wt_1 = pysam.AlignmentFile(m323k_wt_1_bam_path, \"rb\")\n",
    "\n",
    "\n",
    "jnc_counts_dict = {}\n",
    "\n",
    "for transcript, polya_dict in small_polya_jnc_coords.items():\n",
    "    # store junction read counts for every poly(A) site in transcript\n",
    "    tr_polya_dict = {}\n",
    "    #print(polya_dict)\n",
    "    \n",
    "    for polya_site, coords_tuple in polya_dict.items():\n",
    "        \n",
    "        #Reads overlapping poly(A) site\n",
    "        site_jnc_read_count = 0\n",
    "        \n",
    "        for read_entry in wt_1.fetch(coords_tuple[0], coords_tuple[1], coords_tuple[2]):\n",
    "            \n",
    "            #0-based leftmost reference coordinate of the aligned sequence\n",
    "            align_start = read_entry.reference_start\n",
    "            \n",
    "            # Aligned reference position of the read on the reference genome.\n",
    "            # Reference_end points to one past the last aligned residue. Returns None if not available (read is unmapped or no cigar alignment present).\n",
    "\n",
    "            align_end = read_entry.reference_end\n",
    "            \n",
    "            #print(read_entry.query_name)\n",
    "            #print(\"align_start: {0}, align_end: {1}\".format(str(align_start), str(align_end)))\n",
    "            \n",
    "            #polyA site - align_start & align_end - polya_site\n",
    "            if abs(coords_tuple[1] - align_start) > 3 and abs(align_end - coords_tuple[1]) > 3:\n",
    "                site_jnc_read_count += 1\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #Now have checked every read overlapping poly(A) site\n",
    "        tr_polya_dict[polya_site] = site_jnc_read_count\n",
    "        \n",
    "    \n",
    "    #Now have counted junction reads for each poly(A) site in transcript\n",
    "    jnc_counts_dict[transcript] = tr_polya_dict\n",
    "    \n",
    "    \n",
    "\n",
    "print(jnc_counts_dict)\n",
    "\n",
    "\n",
    "def is_polya_junction_read(read_entry,polya_start, sj_overhang):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #0-based leftmost reference coordinate of the aligned sequence\n",
    "    align_start = read_entry.reference_start\n",
    "           \n",
    "    # Aligned reference position of the read on the reference genome.\n",
    "    # Reference_end points to one past the last aligned residue. Returns None if not available (read is unmapped or no cigar alignment present).\n",
    "\n",
    "    align_end = read_entry.reference_end\n",
    "    \n",
    "    if abs(polya_start - align_start) > sj_overhang and abs(align_end - polya_start) > sj_overhang:\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "    \n",
    "#Try a more compartmentalised version of count_dict\n",
    "jnc_counts_dict_2 = {}\n",
    "\n",
    "for transcript, polya_dict in small_polya_jnc_coords.items():\n",
    "    \n",
    "    tr_polya_counts = {polya_site: sum(is_polya_junction_read(read, coords_tuple[1], 3) for read in wt_1.fetch(coords_tuple[0], \n",
    "                                                                            coords_tuple[1], \n",
    "                                                                            coords_tuple[2]))\n",
    "                       for polya_site, coords_tuple in polya_dict.items()}\n",
    "\n",
    "    jnc_counts_dict_2[transcript] = tr_polya_counts\n",
    "\n",
    "    \n",
    "print(jnc_counts_dict_2)\n",
    "\n",
    "wt_1.close()\n",
    "\n",
    "def get_polya_junction_counts_dict(bam_path, jnc_coords_dict, sj_overhang):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    bam = pysam.AlignmentFile(bam_path, \"rb\")\n",
    "    \n",
    "    #final output dict of {tr: {site_id: jnc_counts}}\n",
    "    counts_dict = {}\n",
    "    \n",
    "    for transcript, polya_coords_dict in jnc_coords_dict.items():\n",
    "        \n",
    "        polya_coords_counts = {polya_site: sum(is_polya_junction_read(read,\n",
    "                                                                      coords_tuple[1],\n",
    "                                                                      sj_overhang) \n",
    "                                               \n",
    "                                               for read in bam.fetch(coords_tuple[0], \n",
    "                                                                     coords_tuple[1], \n",
    "                                                                     coords_tuple[2])\n",
    "                                              )\n",
    "                               for polya_site, coords_tuple in polya_coords_dict.items()}\n",
    "        \n",
    "        counts_dict[transcript] = polya_coords_counts\n",
    "        \n",
    "    \n",
    "    bam.close()\n",
    "    \n",
    "    return counts_dict\n",
    "\n",
    "wt_1_polya_jnc_counts = get_polya_junction_counts_dict(m323k_wt_1_bam_path, small_polya_jnc_coords, 3)\n",
    "hom_1_polya_jnc_counts = get_polya_junction_counts_dict(m323k_hom_1_bam_path, small_polya_jnc_coords, 3)\n",
    "\n",
    "print(len(wt_1_polya_jnc_counts))\n",
    "print(len(hom_1_polya_jnc_counts))\n",
    "\n",
    "print(wt_1_polya_jnc_counts)\n",
    "print(hom_1_polya_jnc_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:polya_junctions] *",
   "language": "python",
   "name": "conda-env-polya_junctions-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
